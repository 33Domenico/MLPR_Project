{
 "cells": [
  {
   "attachments": {
    "image.png": {
     "image/png": "iVBORw0KGgoAAAANSUhEUgAAAOgAAAA4CAYAAAD3l7RXAAAAAXNSR0IArs4c6QAAAARnQU1BAACxjwv8YQUAAAAJcEhZcwAAEnQAABJ0Ad5mH3gAABNZSURBVHhe7Z17XEz5/8dfM5PQRW7juu7bRTdfdxtZ7YW1LLGsW7FfSW5LZF1Cym1dIresDZG1JC1WrayILqJyC1EkojvdZ3ammpn39w+36TTVNM2k36/zfDz84f35zJmZ95zX+VzO67zjEBGBhYWlXsJlBlhYWOoPrEBZWOoxrEBZWOoxrEBZWOoxrEBZWOoxrEBZWOoxrEBZWOoxrEBZWOoxrEBZ/v9T+ACxCcXMaC2QIDHsCp5JmfHKKMXjGzeRI2PGq4cVKItmkGUhfP8v2OHjg6NhL5mtdYcwDrvdgyBsr8tsqQVFiNrvi9gSZvwNUokE5bWojc46sdi47kKNRcoKlEUDSPHcZxZ+ftwPxgmeWLDmBLNDHVGAUPedKJ62GDYtNX2qS1CcfgfBOx3w2WfLcK20fGsTSyc4fxKI1X+8ZIi3ajT9qVkaItIkHDt8C4aDB8FyvAeO7J3F7FEnlN70wq6CCZjXrwmzSc1IkRq8GzsCHqAs7yke5DFHUADgoZvddDQ7ugHnC5htlcMKlEX9FMQg5nF39Oqlg06fT8H43i2ZPeoAIUJ9LqPrpJFowWxSOzx0Gb0EaxfbY1hnHXCYze9oYoXpQx7j4KkMBQJWDCtQFjVSgldPH+BOSAQSGrWFXsFDPEoXMDvVDeJwnL3RDcMGKRo9xUiN+B07NmzA9oNnEZdZitKMKPht3YAtvlfwXMzsry60YDzEFE+CQ5DLbKoEVqAs6kNWgJTbNxD6dzSyOraE7G4sbj/Ng9KbnWpEkhSDeANTWDRlNMgyEexiiwVBhP62o8C/6owhfQfCespe5Bq2xeMtYzFiVSTjReqjcZ/+MHx4Azcr2WBiwgqURX1w22LgRHtY6InQadgMzJ7lgGlDO4PH7KckpdFH4BcvYYY/UJqH58kvUPB2Q0YsEOBdb8nTp0gzaIM2jDfP/8sV6wvmwm/bdFibm6F7m8aQZSVBd/xmOHGjcC65BDyevCykeJWUVOPd10rR6Yh29ALP8pU7ICtQFvUiTcW9hGIY9zKDNqNJ8uAg5o0diO6tW6JlWzN8s/EKBADyL6yEVZdW6Nz7K9jO2IkbbwUnSbuDOxnM8VeIxDPrMG/OUqzdfgwRt6Jwcqc7PHevwbSfg/BuQl1WVASpXjM0Zry6kakj9m38Di0BQJaBuFvPIdWywJfDO0F3tCcibiciZvPgDy8Qh2L1uFW4WCR3kNrA0YVe02Lk5ytXJ4EVKIt6KbmPe087w9K8GbMFWuazsO/0cSzpTSjMLUBLQ1PoyPLw8M4D8IZuQ8i1Szjr54xBTGW/Q5aDf1aMxqzzJli62xMeKxdi+qSpcFrmDJN7/sg3+gzv3pXD5UImZYob0DO2Qt92b0/7omuIii8Dr4sVhvbgATw+evbqBn05VUjuR+BmOysM1v8QqxVUipJSLnhKTitYgbKoFcnje3gkM4GFiRaz6Q28HnDcvAi9tTMRuHop3H/6Hq5p/8WxwzNhpsPsLI8UKQdnYlbY59i19wd0LyfiZhgwaCyGDvnk/QndiM+HvrAQQvluAIQJp+CxZD2CXkhREheJOAEHBgMGo7c2AMiQG7Ie64NeAbJ8xJ3cBTePEyjUz8fZ34LwSMl1Y5VQIYpELdGGr5xCWYGyqBXB/Xt43t0SlszNGTka93XB1lmGQPJx7EmdhuN7xqNLJXp+jzgCXluvo8/cOejNnLeCi9aTVsOl74eDaJmYoGv2C6SWW8IKcc59Fjz2+OFSSiGizl5CppSHHmbmaApAlnsVG7feRCvjlgC3BfpPsoextjaGOa3C4nnfoWeF960IEQFEqKzSlyw/DdnNjWCipLGpTgRa+CAWarNClj7GjZs5St9Hqteo1SOqut9TfZTiYXwSDHoPQPeqBoiCh7j9nIfmekDRNX+cTqk4FWUiSY7E9axe+OKrNgpPWq5+83JTU15HawzkxOPWa/mENMGgMWNgbmEO0eFp2CZzhPtUU2SfdsPSpbPxw3Rf8NcewByjtx9efB2RCeb4XOGtGnlkyIvYjzUrXbDo8H1w8y9h25IVWLXOHw8Ye1ziW3eQ3W8oelU2jWdCGkYQu4ucN4VRrpTZoioiivdeSGtDsklth6wWKWVGH6Ndmz1o9cbAKmI1QBBLu5w3UZj6EkOieG9auDaEstV3yJohTaWdNl3ox7+KmS3vkWZepBVfWZOjfxIl7PqCDLg86jDFn7IUfGbhyYW06LyYiIgkCeupv8E4+l3A7EUkzQggn8AsxvlQRnfdhtEEv5wK50lZUQalZhaT5O3/paIcSk3NIRGjY1mcK/W18aQUiZjunvCn2yXvWnLpwEQ78heW7189Qgqdb01zQxR8iUpQdDFSHwWhcN9ZjGmLbaA+K2QTWDo545PA1fjjZV0NF1zof2KOTul/wis0rYqYsmjGI6qq37O2lMQfxFz7Tbiacx0xr77ChC/1mF0AAKUpAZg7eRt4y09h/yQjmDptwaL/NEJmoBs2hVdtaOAZjYWt2SOEXXpV/ruJE3HqcC7MhrdljKxasHBygE7wMSQzBmgt/fbo3E7v/e0fbhM+Onfmownjp+Dot0N7nRxE+Hojhm+t/KhXCbLsv+CfMx4Lv1ZyfqvZKW4pbnrtQsGEeVC7FZLXDXbTm+HohvOoga2xVuh2+hTcolx8OnhIlTFl0JxHVDW/Z+2QIS82CFfymqDwz4totOBnjGCcf5Jb++AwbjgGDZ2Fs8Um6GfOBxeANC0RmZwWaKqVhqNzv8Nk97/xqrIri5YFlvq5Q+/wbCza7o9/rlzCWb+92OYdg04Os2GlYJeV22EK1oxIxN6AdJUuWDzjn3D25ArYTl+C2V92qKVY8hG2OxRGy+fApKrpPxPmkKo2BMHkOHg+hYqYDWqiLJ7ch40ln3TmBEZDlETREpMu5BjyZspVaaxaBBTsOJjmay4xFO8+jMb6pFeY2mkMSQ7dCzlB/mFPqcazviqQn+J+QELF6Ql0I/I6JWQq8W7S1xTmuZJ8bis/rayePDo0ZQYFKPH2byij5JNu5HH6+ftptbLUWqCS/EQKP+NPAcGxlCaXS9Hfs8jC7hQpXI2InlP40e20fr0nHTgTSxklJZQeeYS2rN9Mh8KekXKnrpguzTOlMQdzmA0aQfJoEw1qPZ6O5VUdEz4NpzNBcZRRRkTiNIoNCqRzsen0fvki+ptmWdjRKUWJUUteiMSX5pHpmINUN5nRHIoFqgpiEghqKo2qEeTmKf17EEmpuEi1C0StRm1hrCccl/0DmeUXGGacg0NrDiBJCgASJMXEw8DUAszddllmMFxsFyCI+sN2FB9XnYeg70BrTNmbC8O2j7Fl7AisimQ8TKeQxujT3xAPb9xE5benJCjMTkf6y5d4WeW/dGTkCqvwjMrwOjISSebWGGJQRazwAvb5F6LN/WX4ymE9tntdhsggF8enT8Qvby1rlXlE1ZcX5fyeksJspKcz81DxX3pGLoSVJ0ajcPXboq1epc+G1IDG0NWtybyyenRbtoDyCxQu9PSVX3fKwyFV/zaLLAeHvx+J+0uuY4e1NkrDF2PAz/rYH7UOg7TFOG3XHb42CQh2kH/YJx9n/vsNgm1DcGhsS6A0CkssbLDzpTW8Hgah69quGO/Ph0vYXWwdXP2KvCR0Dsw8LREVMg/vzCHlEEbDe+URxIsIVX9JDnh8GzivmwLF99eFOGNvCI8u53BzQz+86VIxlhewCb/3dMGYi1/D8uhnuHBtCwY+24/5XgJM274EQ1twIT5th+6+NkgIdpB7DEq9eUFJKOaYecIyKgTzFCcG0d4rcSReVOn9undweHzYOK/DFMWJYdEwtRDoC+z92hRu2V9g8g+j8O33k/CtWfO3C+liHBnzKS7YP4P/RHl7iABJ0UkwGNQX7biA7LkXbIxdcP0/6xEfvQpGeY/wQPAJLLrpv1+QS18lIZkMYdym4olWGr0UfZe2QEDkKvRU7wWyPKWRcLaww7+7kuDzzdvrZoWYDLmPElFq1B6XJxpiV+9wXF9j9lbMHyg+MgafXrDHM/+J+JCZmucF0ldISiYYGiu4L1gajaV9l6JFQCRWaTQxgFQqxa+//soMsyiBgYEB7O3tmeFyqC5QACXJZ7Bp3T4EXY3B/dcdMefPaOwZ2QKAAEdtuyFocipOTa7cv1Xwx/foan8O/EVhSPCyrmCuBsS44NQHB62jEWjXnNmIkqs/wXJ1RwSFr8C7e8uaQPpwA6yG3sZPT07D7u2wpygGABBfgKPxAjTxS8CeYRWtJ4KjtugWNBmppybLCbQ81ecFEF9wQp+D1ogOtEOFzJRcxU+Wq9ExKBwrNJkYAGVlZVi8eDEzzKIEfD4fa9euZYbLoZpAZVmI8j2BLKv5mGCqDchyEOxkjeUGh3HH0wraKEGIY0/s6ReD8058uRcKkXDKE4cS+8DZdTiS5hti5IF/Menkc/wxQQ+Q5SJk4z5I5qzCd3wuIImD68DlaB0YiiXdKp5o4r9mwOjQ54g/N1PxU/OCCGyd44XoYln1U9wOY7B+z0yYMYc8yJD127foeWIE7lyyQ1bsKwyyMlEQMwUASOJc0Wf0M6xJPoGJCrb+S0Ic0XNPP8Scd8KHzNQwL5AgznUglrcOROiSbhUf5xL/hRlGh/B5/DnMVJwYRGydA6/oYsiqTgw4vA4Ys34PZlZMDEtdwNg0UgrJky00rG0fWnb53c6UiKJ+/ppmBxW860EpnjbUxzWOyuReR4LjNKEZh7R6LKQruZdoXg8tQqMBtPGRhIik9PryYho2xpuSJFLKi/WnnStHUdceY2j1Dm8695C5myeljL0jyMLl2ocdUo0gpktzu5ORcwSJ0v1os09KJTF6+72tyeDLPZRWyT0OSYon2fRxpTj5xCidFyJpXiz571xJo7r2oDGrd5D3uYdUITMZe2mEhQtd02xiWOoAnru7uztTtNXBbdYKjSWAvj4X/756hIjAI7hqMAvudsbQ4QAAF7paifANEMF2cj+834jj6aHkURhSmrdDRkAItL6fAaPsCETde4nHoYdwIPZTuHo7o68+F007msMoLRBHuLNx/JepMOMzr+AihPvsRuFwV4w3VjQJVBdcNC26haC7QJNMIbpO+gZGejwFMQ4AMa7s88DjAW5YOFTB2hAAV1cLib4BENlORr93iVE6LxxwmnaEuVEaAo9wMfv4L5hqxq+wzhWF+2B34XC4jjdWOD1ucBQ+QOzTpujYpuKSQzUkSAwLR3Hnbmih6EeuQCke37gDaYcO0K3ppjRTscpTRgVpDyk+PokyBQqGi7K75DZsAvnlMNvKqCgjlTKL3zshSZSTSqk5IsaNdREFOxjR+KO55aLvEYbSfOu5VANbYy2QUEHqE0orlv+EimJEoqw0yqly5KrMI6psXohEwQ5kNP4oKc5Mzf2eGkGaSVd/3UTbf/uN/C6/YLbWHRrwPFftxS0jsbjie6nqk66FQKtDSum/T6eJO5Jq7J4gIqKyOHLta0OeKRIS3z1B/h+cykQkpazjDjTR65Fqx/7ISNN/p+kTd1CSSh++jOJc+5KNZwpJxHfphP/tclN8adZxcpjoRY9UOra6kNCzX0dR/8UXKXihIelbbWF2qCPy6eLSqbQhTnlLgXIoEKj4CQWsXkwe+/3o8B4PmufkRkGp8j+ChFIOOJDj0RcVLrhVodQArRpcdJiyBiMS9yIgXQUnJEcf7drrICfCF94xfFjLO5Xzw7A71AjL55hU3CD5P0DtPKIc6LdrD52cCPh6x4Bv3UtuGqui31PdNLi6uCW4uckeOzjTsMJpOn5c4Iatkwrg/uMOucfNVPRJMxWrbqSvw8hzpQ+pZIWUCCivgLEFUpZMJ9086PTzjzpE1J5aeUQlJMgrYGwOqe73VDuvfWl0cyva8uRjfhJNep4ZI2jRGbLvaEbLrsvNZcQh5Ni5O827LP8r1dwnrcER9A3cVjZwWTsVRqpcxHi6aGHAWNhz22LkMjeM6/Ixhwg1wG0FG5e1mKpaYqDbwoBREIuLtiOXwW1cl484q2iYdXFL4yMQk9ca7drLbdfxOqB9q0zERD5+X2mw/tbFbawLtVkhuXpQ0dZYD1GnR1R1v6faaKB1cWVZWcihxtDRkZMTRwe6TQmvs3PKfX9lfNLy1I1AWRoGDbQubplAABFxwSmnJh54XIJQUFzeJMPWxWX5qDTAurgcLgccyCCTyotOCqkM0NZuXP5vtbB1cVk+Kg2wLm6j1ny05JRAJJILkggiMQf6LQzKC5Sti8vyMWmIdXEb9eoD00Z5eC1fr0WajazXTWBqaVJ+JsHWxWX5mDTEurjc9uMw7csC3I7Lfn9fW5oSi3jud7D7tvyfXqyXdXFZGgoNtC4utz3sd2xF52BX7LycjJyXEdjhEYUBOzfDtlX5rvWuLi5LA6Kh18UVpVFs0Ak6EXSDUgoUmTRq7pNWdDFiYakRbF3ctzTpiP6jJ2Py6IHoZlBxClHP6uKyNAzYurjKoaJPmjmksrDUGLYubjWo7pNWreQJC0sd8G/AIrjqb8XOkZVsnypNCYRCLTXaKgFhXj54SpfelEFQLFLJilm7UZuFRYOwdXFVLRrGwsJSJ7AjKAtLPYYVKAtLPYYVKAtLPeZ/jizo9MAleu8AAAAASUVORK5CYII="
    }
   },
   "cell_type": "markdown",
   "id": "7798e430",
   "metadata": {},
   "source": [
    "###  Gaussian Classifier\n",
    "\n",
    "Apply the MVG model to the project data. Split the dataset in model training and validation subsets\n",
    "(important: use the same splits for all models, including those presented in other laboratories), train the\n",
    "model parameters on the model training portion of the dataset and compute LLRs\n",
    "\n",
    "![image.png](attachment:image.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "id": "d5cd8928",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.linalg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "id": "ed4c6e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vcol(x):\n",
    "    return x.reshape((x.size, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "id": "bc04812c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vrow(x):\n",
    "    return x.reshape((1, x.size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "id": "fdf4ae3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(filename):\n",
    "    DList=[]\n",
    "    lablesList=[]\n",
    "    #La classe puo cambiare\n",
    "    with open(filename) as f:\n",
    "        for line in f:\n",
    "            line=line.strip()\n",
    "            try:\n",
    "                fields=line.split(',')[0:-1] # prendo tutti i campi tranne l'ultimo ovvero l etichetta\n",
    "                fields=vcol(np.array([float(x) for x in fields]))\n",
    "                DList.append(fields)\n",
    "                label=line.split(',')[-1].strip()\n",
    "                lablesList.append(label)\n",
    "            except:\n",
    "                pass\n",
    "    return np.hstack(DList),np.array(lablesList,dtype=np.int32)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "id": "8e441cfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#prima di essre processati i dati devono essere spostati nell origini, ovvero avere media 0\n",
    "def compute_mu_C(X):\n",
    "    \"Calcolo la media e la matrice di covarianza dei dati x\"\n",
    "    mu = vcol(X.mean(1))\n",
    "\n",
    "    C = ((X-mu)@(X-mu).T)/float(X.shape[1])\n",
    "    return mu, C"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "58495d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def logpdf_GAU_ND(X, mu, C):\n",
    "    \"\"\"\n",
    "    Compute the log-density of a multivariate Gaussian distribution for multiple samples.\n",
    "    X is a 2D numpy array where each column is a sample, mu is the mean vector, and C is the covariance matrix.\n",
    "    Returns a 1D numpy array of log-density values for each sample.\n",
    "    \"\"\"\n",
    "    P=np.linalg.inv(C) #inverse of covariance matrix (Precision matrix)\n",
    "    return -0.5*X.shape[0]*np.log(2*np.pi)-0.5*np.linalg.slogdet(C)[1]-0.5* ((X-mu) * (P @ (X-mu))).sum(0) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "id": "99a6633a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_db_2to1(D, L, seed=0):\n",
    "    nTrain = int(D.shape[1]*2.0/3.0)\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.permutation(D.shape[1])\n",
    "    idxTrain = idx[0:nTrain]\n",
    "    idxTest = idx[nTrain:]\n",
    "    DTR = D[:, idxTrain]\n",
    "    DVAL = D[:, idxTest]\n",
    "    LTR = L[idxTrain]\n",
    "    LVAL = L[idxTest]\n",
    "    return (DTR, LTR), (DVAL, LVAL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "id": "9b1cfec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "D,lables=load('trainData.txt')\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da64c7b5",
   "metadata": {},
   "source": [
    "(i.e., with class True, label 1 on top of the ratio) for the validation subset. Obtain predictions from\n",
    "LLRs assuming uniform class priors P(C = 1) = P(C = 0) = 1/2. Compute the corresponding error\n",
    "rate (suggestion: in the next laboratories we will modify the way we compute predictions from LLRs,\n",
    "we therefore recommend that you keep separated the functions that compute LLRs, those that compute\n",
    "predictions from LLRs and those that compute error rate from predictions).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaf8bce7",
   "metadata": {},
   "source": [
    "### MVG Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 386,
   "id": "d744fad7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gau_MVG_ML_estimation(D,L):\n",
    "    labelSet=set(L)\n",
    "    hParams = {}\n",
    "    for label in labelSet:\n",
    "        DLabel=D[:,L==label]\n",
    "        mu, C = compute_mu_C(DLabel)\n",
    "        hParams[label] = (mu, C)\n",
    "    return hParams\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed7228e",
   "metadata": {},
   "source": [
    "### Tied Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 387,
   "id": "a46178f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gau_Tied_ML_estimation(D,L):\n",
    "    labelSet=set(L)\n",
    "    hParams={}\n",
    "    hMeans={}\n",
    "    CGlobal=0\n",
    "    for label in labelSet:\n",
    "        DLabel=D[:,L==label]\n",
    "        mu, C_class = compute_mu_C(DLabel)\n",
    "        hMeans[label] = mu\n",
    "        CGlobal += C_class*DLabel.shape[1]\n",
    "    CGlobal=CGlobal/D.shape[1]\n",
    "    for label in labelSet:\n",
    "        hParams[label] = (hMeans[label], CGlobal)\n",
    "    return hParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 388,
   "id": "54fb4103",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_llr_binary(D,hParams):\n",
    "    \"\"\"Compute log-likelihood ratio (LLR) for binary classification.\"\"\"\n",
    "    llr = logpdf_GAU_ND(D, hParams[1][0], hParams[1][1]) - logpdf_GAU_ND(D, hParams[0][0], hParams[0][1])\n",
    "    return llr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 389,
   "id": "6af276ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_prediction_from_llr_binary(llr,trashold=0):\n",
    "    \"\"\"Compute binary prediction from log-likelihood ratio (LLR).\"\"\"\n",
    "    predictions=np.zeros(llr.size,dtype=np.int32)\n",
    "    predictions[llr>=trashold]=1\n",
    "    predictions[llr<trashold]=0\n",
    "    print('Number of errors:',np.sum(predictions!=LVAL),\" out of \",LVAL.size,\"samples\")\n",
    "    return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 390,
   "id": "9c66ed50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_error_rate(predictions, labels):\n",
    "    \"\"\"Compute error rate.\"\"\"\n",
    "    return (predictions != labels).sum() / float(labels.size) * 100"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b297aa",
   "metadata": {},
   "source": [
    "Apply now the tied Gaussian model, and compare the results with MVG and LDA. Which model seems\n",
    "to perform better?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "id": "e7ca9316",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 140  out of  2000 samples\n",
      "Error rate MVG:  7.000000000000001\n",
      "Accuracy MVG:  93.0\n",
      "\n",
      "Number of errors: 186  out of  2000 samples\n",
      "Error rate Tied:  9.3\n",
      "Accuracy Tied:  90.7\n"
     ]
    }
   ],
   "source": [
    "#MVG model\n",
    "hParams_mvg = Gau_MVG_ML_estimation(DTR, LTR)\n",
    "llr_mvg = compute_llr_binary(DVAL, hParams_mvg)\n",
    "predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "print(\"\")\n",
    "#Tied model\n",
    "hParams_tied = Gau_Tied_ML_estimation(DTR, LTR)\n",
    "llr_tied = compute_llr_binary(DVAL, hParams_tied)\n",
    "predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "print(\"Error rate Tied: \", error_rate_tied)\n",
    "print(\"Accuracy Tied: \", 100 - error_rate_tied)    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b97fa7",
   "metadata": {},
   "source": [
    "Finally, test the Naive Bayes Gaussian model. How does it compare with the previous two?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06d1b86b",
   "metadata": {},
   "source": [
    "### Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "id": "ab691915",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Gau_NaiveBayes_ML_estimation(D,L):\n",
    "    labelSet=set(L)\n",
    "    hParams = {}\n",
    "    for label in labelSet:\n",
    "        DLabel=D[:,L==label]\n",
    "        mu, C = compute_mu_C(DLabel)\n",
    "        C = C*np.eye(D.shape[0]) #diagonal covariance matrix\n",
    "        hParams[label] = (mu, C)\n",
    "    return hParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "id": "5b7e1ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 144  out of  2000 samples\n",
      "Error rate Naive:  7.199999999999999\n",
      "Accuracy Naive:  92.8\n"
     ]
    }
   ],
   "source": [
    "#Naive Bayes model\n",
    "hParams_naive = Gau_NaiveBayes_ML_estimation(DTR, LTR)\n",
    "llr_naive = compute_llr_binary(DVAL, hParams_naive)\n",
    "predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "print(\"Error rate Naive: \", error_rate_naive)\n",
    "print(\"Accuracy Naive: \", 100 - error_rate_naive)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "015db597",
   "metadata": {},
   "source": [
    "The MVG (Multivariate Gaussian) model resulted in 140 errors out of 2000 samples, giving an error rate of 7.0% and an accuracy of 93.0%.\n",
    "\n",
    "The Tied Gaussian model resulted in 186 errors out of 2000 samples, with an error rate of 9.3% and accuracy of 90.7%.\n",
    "\n",
    "The LDA model showed the same performance as the Tied Gaussian model with an error rate of 9.3% and accuracy of 90.7%.(Part 2)\n",
    "\n",
    "The Naive Bayes Gaussian model resulted in 144 errors out of 2000 samples, with an error rate of 7.2% and accuracy of 92.8%.\n",
    "\n",
    "Comparing these models:\n",
    "\n",
    "1. The MVG model performs the best with 93.0% accuracy, closely followed by the Naive Bayes model with 92.8% accuracy.\n",
    "\n",
    "2. Both the Tied Gaussian and LDA models perform identically with 90.7% accuracy. This is expected since these models are mathematically equivalent when using the same threshold - both assume a common covariance matrix for all classes.\n",
    "\n",
    "3. The superior performance of the MVG model suggests that the classes in your dataset have different covariance structures, which the MVG model successfully captures by estimating separate covariance matrices for each class.\n",
    "\n",
    "4. The Naive Bayes model performs remarkably well despite its simplified assumption of feature independence. This suggests that while there might be some correlation between features, modeling each feature independently still captures most of the discriminative information in the data.\n",
    "\n"
   ]
  },
  {
   "attachments": {
    "image.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "id": "eb17dd7d",
   "metadata": {},
   "source": [
    "### Covariance Matrix Analysis\n",
    "Let’s now analyze the results in light of the characteristics of the features that we observed in previous\n",
    "laboratories. Start by printing the covariance matrix of each class (you can extract this from the MVG\n",
    "model parameters). The covariance matrices contain, on the diagonal, the variances for the different\n",
    "features, whereas the elements outside of the diagonal are the feature co-variances. For each class,\n",
    "compare the covariance of different feature pairs with the respective variances. What do you observe?\n",
    "Are co-variance values large or small compared to variances? To better visualize the strength of covariances with respect to variances we can compute, for a pair of features i, j, the Pearson correlation\n",
    "coefficient, defined as:\n",
    "\n",
    "![image.png](attachment:image.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7920ca",
   "metadata": {},
   "source": [
    "Compute the correlation matrices for the two classes. What can you conclude on the features? Are the\n",
    "features strongly or weakly correlated? How is this related to the Naive Bayes results?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "id": "20d33388",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_correlation_matrix(C):\n",
    "    \"\"\"Compute the correlation matrix from the covariance matrix.\"\"\"\n",
    "    Corr = C / ( vcol(C.diagonal()**0.5) * vrow(C.diagonal()**0.5) )\n",
    "    return Corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "id": "07a879c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Covariance matrix for class 0:\n",
      "[[ 6.00956506e-01  5.15866517e-05  1.90589145e-02  1.92529876e-02\n",
      "   1.28039402e-02 -1.34721598e-02]\n",
      " [ 5.15866517e-05  1.44722543e+00 -1.61340110e-02 -1.58561474e-02\n",
      "  -2.64529141e-02  2.29139833e-02]\n",
      " [ 1.90589145e-02 -1.61340110e-02  5.65348901e-01 -1.84344435e-03\n",
      "  -6.91446277e-03  1.68928322e-02]\n",
      " [ 1.92529876e-02 -1.58561474e-02 -1.84344435e-03  5.41615202e-01\n",
      "   5.25171375e-03  1.35717775e-02]\n",
      " [ 1.28039402e-02 -2.64529141e-02 -6.91446277e-03  5.25171375e-03\n",
      "   6.96067641e-01  1.58438399e-02]\n",
      " [-1.34721598e-02  2.29139833e-02  1.68928322e-02  1.35717775e-02\n",
      "   1.58438399e-02  6.86519710e-01]]\n",
      "\n",
      "Correlation matrix for class 0:\n",
      "[[ 1.00000000e+00  5.53156127e-05  3.26977873e-02  3.37466904e-02\n",
      "   1.97968638e-02 -2.09743833e-02]\n",
      " [ 5.53156127e-05  1.00000000e+00 -1.78367604e-02 -1.79095288e-02\n",
      "  -2.63560127e-02  2.29882544e-02]\n",
      " [ 3.26977873e-02 -1.78367604e-02  1.00000000e+00 -3.33139656e-03\n",
      "  -1.10223563e-02  2.71155043e-02]\n",
      " [ 3.37466904e-02 -1.79095288e-02 -3.33139656e-03  1.00000000e+00\n",
      "   8.55322509e-03  2.22569065e-02]\n",
      " [ 1.97968638e-02 -2.63560127e-02 -1.10223563e-02  8.55322509e-03\n",
      "   1.00000000e+00  2.29196624e-02]\n",
      " [-2.09743833e-02  2.29882544e-02  2.71155043e-02  2.22569065e-02\n",
      "   2.29196624e-02  1.00000000e+00]]\n",
      "\n",
      "Covariance matrix for class 1:\n",
      "[[ 1.44809527e+00 -1.47222433e-02  5.57010301e-03  1.57415883e-02\n",
      "   1.94971163e-02 -1.76682539e-04]\n",
      " [-1.47222433e-02  5.53390796e-01 -1.12168681e-02 -9.06473359e-03\n",
      "  -1.46589901e-02  1.63492048e-02]\n",
      " [ 5.57010301e-03 -1.12168681e-02  5.57480229e-01  2.75609663e-02\n",
      "  -3.76966451e-03 -1.45976943e-02]\n",
      " [ 1.57415883e-02 -9.06473359e-03  2.75609663e-02  5.69657013e-01\n",
      "  -1.16983404e-02  3.49931863e-02]\n",
      " [ 1.94971163e-02 -1.46589901e-02 -3.76966451e-03 -1.16983404e-02\n",
      "   1.34201767e+00  1.69454096e-02]\n",
      " [-1.76682539e-04  1.63492048e-02 -1.45976943e-02  3.49931863e-02\n",
      "   1.69454096e-02  1.30371880e+00]]\n",
      "\n",
      "Correlation matrix for class 1:\n",
      "[[ 1.00000000e+00 -1.64459687e-02  6.19940380e-03  1.73317836e-02\n",
      "   1.39859734e-02 -1.28588787e-04]\n",
      " [-1.64459687e-02  1.00000000e+00 -2.01948630e-02 -1.61447883e-02\n",
      "  -1.70101823e-02  1.92481371e-02]\n",
      " [ 6.19940380e-03 -2.01948630e-02  1.00000000e+00  4.89072205e-02\n",
      "  -4.35821698e-03 -1.71229097e-02]\n",
      " [ 1.73317836e-02 -1.61447883e-02  4.89072205e-02  1.00000000e+00\n",
      "  -1.33794547e-02  4.06054941e-02]\n",
      " [ 1.39859734e-02 -1.70101823e-02 -4.35821698e-03 -1.33794547e-02\n",
      "   1.00000000e+00  1.28109397e-02]\n",
      " [-1.28588787e-04  1.92481371e-02 -1.71229097e-02  4.06054941e-02\n",
      "   1.28109397e-02  1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "C = hParams_mvg[0][1]  # Covariance matrix for class 0\n",
    "C1 = hParams_mvg[1][1]  # Covariance matrix for class 1\n",
    "    \n",
    "print(\"Covariance matrix for class 0:\")\n",
    "print(C0)\n",
    "print(\"\\nCorrelation matrix for class 0:\")\n",
    "print(compute_correlation_matrix(C0))\n",
    "print(\"\\nCovariance matrix for class 1:\")\n",
    "print(C1)\n",
    "print(\"\\nCorrelation matrix for class 1:\")\n",
    "print(compute_correlation_matrix(C1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ee2c28",
   "metadata": {},
   "source": [
    "Looking at the correlation matrices, we can observe that all off-diagonal elements have very small values, with most correlation coefficients falling between -0.05 and 0.05. The largest correlation coefficient is approximately 0.049 (between features 3 and 4 in class 1).\n",
    "\n",
    "These small correlation values indicate that the features in your dataset are very weakly correlated. In fact, they are nearly uncorrelated, as correlation coefficients close to zero suggest minimal linear relationship between features.\n",
    "\n",
    "This weak correlation between features explains why the Naive Bayes model performs almost as well as the full MVG model (92.8% vs 93.0% accuracy). The Naive Bayes model makes the assumption that features are independent within each class, which is represented by diagonal covariance matrices. When features are truly nearly independent (as indicated by the correlation matrices), this assumption is very close to reality, and the Naive Bayes model suffers minimal performance loss compared to the full model.\n",
    "\n",
    "The MVG model still performs slightly better (by 0.2%) because it can capture even these weak correlations between features, which apparently provide some small additional discriminative information. However, the difference is minimal, suggesting that the independence assumption is a reasonable simplification for this dataset.\n",
    "\n",
    "This also explains why the Naive Bayes model significantly outperforms the Tied Gaussian model (92.8% vs 90.7%). The Naive Bayes model correctly captures the class-specific variances while making a reasonable assumption about feature independence, whereas the Tied model incorrectly assumes that both classes share the same covariance structure.\n",
    "\n",
    "In conclusion, the features in your dataset exhibit very weak correlation, which aligns perfectly with the strong performance of the Naive Bayes model. This suggests that for this classification task, the diagonal covariance assumption is appropriate, making Naive Bayes an efficient model choice that provides a good balance between performance and computational simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5d5a5ca",
   "metadata": {},
   "source": [
    "The Gaussian model assumes that features can be jointly modeled by Gaussian distributions. The goodness of the model is therefore strongly affected by the accuracy of this assumption. Although visualizing\n",
    "6-dimensional distributions is unfeasible, we can analyze how well the assumption holds for single (or\n",
    "pairs) of features. In Part 3 we separately fitted a Gaussian density over each feature for each\n",
    "class. This corresponds to the Naive Bayes model. What can you conclude on the goodness of the\n",
    "Gaussian assumption? Is it accurate for all the 6 features? Are there features for which the assumptions\n",
    "do not look good?\n",
    "\n",
    "As observed in Part 3, Features 4 and 5 for both classes deviate significantly from Gaussian distribution assumptions. The multimodal nature of their distributions, particularly the bimodal pattern in the True Fingerprint class, fundamentally contradicts the unimodal character expected in Gaussian models.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa06483",
   "metadata": {},
   "source": [
    "To analyze if indeed the last set of features negatively affects our classifier because of poor modeling\n",
    "assumptions, we can try repeating the classification using only feature 1 to 4 (i.e., discarding the last 2\n",
    "features). Repeat the analysis for the three models. What do you obtain? What can we conclude on\n",
    "discarding the last two features? Despite the inaccuracy of the assumption for these two features, are\n",
    "the Gaussian models still able to extract some useful information to improve classification accuracy?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "id": "74c2a2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 159  out of  2000 samples\n",
      "Error rate MVG:  7.95\n",
      "Accuracy MVG:  92.05\n",
      "\n",
      "Number of errors: 190  out of  2000 samples\n",
      "Error rate Tied:  9.5\n",
      "Accuracy Tied:  90.5\n",
      "\n",
      "Number of errors: 153  out of  2000 samples\n",
      "Error rate Naive:  7.6499999999999995\n",
      "Accuracy Naive:  92.35\n"
     ]
    }
   ],
   "source": [
    "#Fetures 0 to 3\n",
    "D,lables=load('trainData.txt')\n",
    "D=D[0:4,:]\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n",
    "\n",
    "#MVG model\n",
    "hParams_mvg = Gau_MVG_ML_estimation(DTR, LTR)\n",
    "llr_mvg = compute_llr_binary(DVAL, hParams_mvg)\n",
    "predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "print(\"\")\n",
    "#Tied model\n",
    "hParams_tied = Gau_Tied_ML_estimation(DTR, LTR)\n",
    "llr_tied = compute_llr_binary(DVAL, hParams_tied)\n",
    "predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "print(\"Error rate Tied: \", error_rate_tied)\n",
    "print(\"Accuracy Tied: \", 100 - error_rate_tied)\n",
    "print(\"\")    \n",
    "#Naive Bayes model\n",
    "hParams_naive = Gau_NaiveBayes_ML_estimation(DTR, LTR)\n",
    "llr_naive = compute_llr_binary(DVAL, hParams_naive)\n",
    "predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "print(\"Error rate Naive: \", error_rate_naive)\n",
    "print(\"Accuracy Naive: \", 100 - error_rate_naive)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "id": "18e6d2de",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 542  out of  2000 samples\n",
      "Error rate MVG:  27.1\n",
      "Accuracy MVG:  72.9\n",
      "\n",
      "Number of errors: 979  out of  2000 samples\n",
      "Error rate Tied:  48.949999999999996\n",
      "Accuracy Tied:  51.050000000000004\n",
      "\n",
      "Number of errors: 524  out of  2000 samples\n",
      "Error rate Naive:  26.200000000000003\n",
      "Accuracy Naive:  73.8\n"
     ]
    }
   ],
   "source": [
    "#Feture 4 to 5\n",
    "D,lables=load('trainData.txt')\n",
    "D=D[4:,:]\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n",
    "\n",
    "#MVG model\n",
    "hParams_mvg = Gau_MVG_ML_estimation(DTR, LTR)\n",
    "llr_mvg = compute_llr_binary(DVAL, hParams_mvg)\n",
    "predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "print(\"\")\n",
    "#Tied model\n",
    "hParams_tied = Gau_Tied_ML_estimation(DTR, LTR)\n",
    "llr_tied = compute_llr_binary(DVAL, hParams_tied)\n",
    "predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "print(\"Error rate Tied: \", error_rate_tied)\n",
    "print(\"Accuracy Tied: \", 100 - error_rate_tied)\n",
    "print(\"\")    \n",
    "#Naive Bayes model\n",
    "hParams_naive = Gau_NaiveBayes_ML_estimation(DTR, LTR)\n",
    "llr_naive = compute_llr_binary(DVAL, hParams_naive)\n",
    "predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "print(\"Error rate Naive: \", error_rate_naive)\n",
    "print(\"Accuracy Naive: \", 100 - error_rate_naive)\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446149b",
   "metadata": {},
   "source": [
    "When using only the first 4 features (discarding the last 2 features):\n",
    "- MVG: 7.95% error rate (92.05% accuracy)\n",
    "- Tied: 9.5% error rate (90.5% accuracy)\n",
    "- Naive Bayes: 7.65% error rate (92.35% accuracy)\n",
    "\n",
    "When using only the last 2 features:\n",
    "- MVG: 27.1% error rate (72.9% accuracy)\n",
    "- Tied: 48.95% error rate (51.05% accuracy)\n",
    "- Naive Bayes: 26.2% error rate (73.8% accuracy)\n",
    "\n",
    "When comparing these results to your original full-feature model results (MVG: 7.0%, Tied: 9.3%, Naive Bayes: 7.2%), we can make several observations:\n",
    "\n",
    "1. Using only the first 4 features yields performance that is quite close to using all 6 features. The error rates increased only slightly (by about 0.5-1.0%). This suggests that the first 4 features contain most of the discriminative information.\n",
    "\n",
    "2. Using only the last 2 features results in significantly worse performance, with error rates increasing dramatically (to 26-49%). However, these features still provide classification accuracy well above random chance (50% for binary classification), which indicates they do contain some useful discriminative information.\n",
    "\n",
    "3. The poor performance when using only the last 2 features might suggest these features don't follow Gaussian distributions as well as the first 4 features. However, the Naive Bayes model actually performs better than MVG on these 2 features (73.8% vs 72.9% accuracy), which is interesting.\n",
    "\n",
    "To answer your specific questions:\n",
    "\n",
    "1. **What can we conclude about discarding the last two features?** \n",
    "Discarding the last two features results in only a minor decrease in performance. This suggests these features contribute only marginally to the classification task when used alongside the first four features. If computational efficiency is a concern, you could reasonably use only the first 4 features with minimal impact on performance.\n",
    "\n",
    "2. **Despite the potential inaccuracy of the Gaussian assumption for these two features, are the models still able to extract useful information?**\n",
    "Yes, definitely. Even though the performance using only these two features is much worse than using all features, the models still achieve accuracy substantially better than random chance (73.8% at best versus 50% for random guessing). This indicates that despite any potential deviation from Gaussian distribution, these features do contain useful discriminative information that the models can leverage.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da13597",
   "metadata": {},
   "source": [
    "In Part 2 and 3 we analyzed the distribution of features 0-1 and of features 2-3, finding that for\n",
    "features 0 and 1 means are similar but variances are not, whereas for features 2 and 3 the two classes\n",
    "mainly differ for the feature mean, but show similar variance. Furthermore, the features also show limited\n",
    "correlation for both classes. We can analyze how these characteristics of the features distribution affect\n",
    "the performance of the different approaches. Repeat the classification using only features 0-1 (jointly),\n",
    "and then do the same using only features 2-3 (jointly), and compare the results of the MVG and tied\n",
    "MVG models. In the first case, which model is better? And in the second case? How is this related\n",
    "to the characteristics of the two classifiers? Is the tied model effective at all for the first two features?\n",
    "Why? And the MVG? And for the second pair of features?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "id": "0da974f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 730  out of  2000 samples\n",
      "Error rate MVG:  36.5\n",
      "Accuracy MVG:  63.5\n",
      "\n",
      "Number of errors: 989  out of  2000 samples\n",
      "Error rate Tied:  49.45\n",
      "Accuracy Tied:  50.55\n",
      "\n",
      "Number of errors: 726  out of  2000 samples\n",
      "Error rate Naive:  36.3\n",
      "Accuracy Naive:  63.7\n"
     ]
    }
   ],
   "source": [
    "#Select only feature 0 and 1\n",
    "D,lables=load('trainData.txt')\n",
    "D=D[:2,:]\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n",
    "\n",
    "#MVG model\n",
    "hParams_mvg = Gau_MVG_ML_estimation(DTR, LTR)\n",
    "llr_mvg = compute_llr_binary(DVAL, hParams_mvg)\n",
    "predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "print(\"\")\n",
    "#Tied model\n",
    "hParams_tied = Gau_Tied_ML_estimation(DTR, LTR)\n",
    "llr_tied = compute_llr_binary(DVAL, hParams_tied)\n",
    "predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "print(\"Error rate Tied: \", error_rate_tied)\n",
    "print(\"Accuracy Tied: \", 100 - error_rate_tied)\n",
    "print(\"\")    \n",
    "#Naive Bayes model\n",
    "hParams_naive = Gau_NaiveBayes_ML_estimation(DTR, LTR)\n",
    "llr_naive = compute_llr_binary(DVAL, hParams_naive)\n",
    "predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "print(\"Error rate Naive: \", error_rate_naive)\n",
    "print(\"Accuracy Naive: \", 100 - error_rate_naive)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "id": "e7ce380a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of errors: 189  out of  2000 samples\n",
      "Error rate MVG:  9.45\n",
      "Accuracy MVG:  90.55\n",
      "\n",
      "Number of errors: 188  out of  2000 samples\n",
      "Error rate Tied:  9.4\n",
      "Accuracy Tied:  90.6\n",
      "\n",
      "Number of errors: 189  out of  2000 samples\n",
      "Error rate Naive:  9.45\n",
      "Accuracy Naive:  90.55\n"
     ]
    }
   ],
   "source": [
    "#Select Feature 2 and 3\n",
    "D,lables=load('trainData.txt')\n",
    "D=D[2:4,:]\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n",
    "\n",
    "#MVG model\n",
    "hParams_mvg = Gau_MVG_ML_estimation(DTR, LTR)\n",
    "llr_mvg = compute_llr_binary(DVAL, hParams_mvg)\n",
    "predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "print(\"\")\n",
    "#Tied model\n",
    "hParams_tied = Gau_Tied_ML_estimation(DTR, LTR)\n",
    "llr_tied = compute_llr_binary(DVAL, hParams_tied)\n",
    "predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "print(\"Error rate Tied: \", error_rate_tied)\n",
    "print(\"Accuracy Tied: \", 100 - error_rate_tied)\n",
    "print(\"\")    \n",
    "#Naive Bayes model\n",
    "hParams_naive = Gau_NaiveBayes_ML_estimation(DTR, LTR)\n",
    "llr_naive = compute_llr_binary(DVAL, hParams_naive)\n",
    "predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "print(\"Error rate Naive: \", error_rate_naive)\n",
    "print(\"Accuracy Naive: \", 100 - error_rate_naive)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae2c8b78",
   "metadata": {},
   "source": [
    "For features 0-1:\n",
    "- MVG: 36.5% error rate (63.5% accuracy)\n",
    "- Tied: 49.45% error rate (50.55% accuracy)\n",
    "- Naive Bayes: 36.3% error rate (63.7% accuracy)\n",
    "\n",
    "For features 2-3:\n",
    "- MVG: 9.45% error rate (90.55% accuracy)\n",
    "- Tied: 9.4% error rate (90.6% accuracy)\n",
    "- Naive Bayes: 9.45% error rate (90.55% accuracy)\n",
    "\n",
    "These results reveal important insights about both the feature characteristics and the classifiers:\n",
    "\n",
    "1. **Features 0-1 (means are similar but variances differ)**:\n",
    "   - The Tied model performs extremely poorly (barely better than random guessing at 50.55% accuracy)\n",
    "   - Both MVG and Naive Bayes perform much better (around 63.5-63.7% accuracy)\n",
    "   - Naive Bayes slightly outperforms MVG\n",
    "\n",
    "   This makes perfect sense because the Tied model assumes the same covariance matrix for both classes. When the key discriminative information is in the different variances between classes (as in features 0-1), forcing the same covariance matrix effectively discards this crucial information. Both MVG and Naive Bayes can capture class-specific variances, allowing them to perform much better.\n",
    "\n",
    "2. **Features 2-3 (means differ but variances are similar)**:\n",
    "   - All three models perform similarly well (around 90.5-90.6% accuracy)\n",
    "   - The Tied model actually performs slightly better than the others\n",
    "   \n",
    "   This also aligns perfectly with the feature characteristics. When the main discriminative information lies in the difference between class means (rather than variances), the Tied model doesn't lose any important information by assuming a common covariance matrix. In fact, by reducing the number of parameters to estimate, the Tied model might achieve a slightly more stable estimate of the covariance structure.\n",
    "\n",
    "To answer your specific questions:\n",
    "\n",
    "- **For features 0-1, which model is better?** \n",
    "  Naive Bayes slightly outperforms MVG, and both dramatically outperform the Tied model. This is because features 0-1 have different variances between classes, which only MVG and Naive Bayes can capture.\n",
    "\n",
    "- **For features 2-3, which model is better?**\n",
    "  All models perform similarly, with the Tied model having a slight edge. This is because the discriminative information is in the means, not the covariances.\n",
    "\n",
    "- **Is the Tied model effective at all for the first two features (0-1)?**\n",
    "  No, it performs terribly (50.55% accuracy, barely above random chance). This is because the Tied model cannot capture the different variances between classes, which is the primary discriminative information for these features.\n",
    "\n",
    "- **Why does MVG work for features 0-1?**\n",
    "  MVG works well because it estimates separate covariance matrices for each class, allowing it to capture the crucial variance differences between classes.\n",
    "\n",
    "- **For the second pair of features (2-3), why do all models perform similarly?**\n",
    "  Because the discriminative information is mainly in the means, not the covariances. When class variances are similar, the Tied model's assumption of a common covariance matrix is actually appropriate and doesn't discard useful information.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15eff692",
   "metadata": {},
   "source": [
    "Finally, we can analyze the effects of PCA as pre-processing. Use PCA to reduce the dimensionality of\n",
    "the feature space, and apply the three classification approaches. What do you observe? Is PCA effective\n",
    "for this dataset with the Gaussian models? Overall, which is the model that provided the best accuracy\n",
    "on the validation set?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 400,
   "id": "c5df2980",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pca(X,m):\n",
    "    mu,C=compute_mu_C(X)\n",
    "    U,s,Vh=scipy.linalg.svd(C) #ritorna autovalori e autovettori dal più grande al più piccolo\n",
    "    P=U[:,0:m] #prendo le prime m colonne\n",
    "    return P"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 401,
   "id": "a2929a1d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m= 1\n",
      "Number of errors: 185  out of  2000 samples\n",
      "Error rate MVG:  9.25\n",
      "Accuracy MVG:  90.75\n",
      "\n",
      "Number of errors: 187  out of  2000 samples\n",
      "Error rate Tied:  9.35\n",
      "Accuracy Tied:  90.65\n",
      "\n",
      "Number of errors: 185  out of  2000 samples\n",
      "Error rate Naive:  9.25\n",
      "Accuracy Naive:  90.75\n",
      "\n",
      "m= 2\n",
      "Number of errors: 176  out of  2000 samples\n",
      "Error rate MVG:  8.799999999999999\n",
      "Accuracy MVG:  91.2\n",
      "\n",
      "Number of errors: 185  out of  2000 samples\n",
      "Error rate Tied:  9.25\n",
      "Accuracy Tied:  90.75\n",
      "\n",
      "Number of errors: 177  out of  2000 samples\n",
      "Error rate Naive:  8.85\n",
      "Accuracy Naive:  91.15\n",
      "\n",
      "m= 3\n",
      "Number of errors: 176  out of  2000 samples\n",
      "Error rate MVG:  8.799999999999999\n",
      "Accuracy MVG:  91.2\n",
      "\n",
      "Number of errors: 185  out of  2000 samples\n",
      "Error rate Tied:  9.25\n",
      "Accuracy Tied:  90.75\n",
      "\n",
      "Number of errors: 180  out of  2000 samples\n",
      "Error rate Naive:  9.0\n",
      "Accuracy Naive:  91.0\n",
      "\n",
      "m= 4\n",
      "Number of errors: 161  out of  2000 samples\n",
      "Error rate MVG:  8.05\n",
      "Accuracy MVG:  91.95\n",
      "\n",
      "Number of errors: 185  out of  2000 samples\n",
      "Error rate Tied:  9.25\n",
      "Accuracy Tied:  90.75\n",
      "\n",
      "Number of errors: 177  out of  2000 samples\n",
      "Error rate Naive:  8.85\n",
      "Accuracy Naive:  91.15\n",
      "\n",
      "m= 5\n",
      "Number of errors: 142  out of  2000 samples\n",
      "Error rate MVG:  7.1\n",
      "Accuracy MVG:  92.9\n",
      "\n",
      "Number of errors: 186  out of  2000 samples\n",
      "Error rate Tied:  9.3\n",
      "Accuracy Tied:  90.7\n",
      "\n",
      "Number of errors: 175  out of  2000 samples\n",
      "Error rate Naive:  8.75\n",
      "Accuracy Naive:  91.25\n",
      "\n",
      "m= 6\n",
      "Number of errors: 140  out of  2000 samples\n",
      "Error rate MVG:  7.000000000000001\n",
      "Accuracy MVG:  93.0\n",
      "\n",
      "Number of errors: 186  out of  2000 samples\n",
      "Error rate Tied:  9.3\n",
      "Accuracy Tied:  90.7\n",
      "\n",
      "Number of errors: 178  out of  2000 samples\n",
      "Error rate Naive:  8.9\n",
      "Accuracy Naive:  91.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "D,lables=load('trainData.txt')\n",
    "(DTR,LTR), (DVAL,LVAL)=split_db_2to1(D,lables)\n",
    "m_try=[1,2,3,4,5,6]\n",
    "for m in m_try:\n",
    "    print(\"m=\",m)\n",
    "    #PCA\n",
    "    P=compute_pca(DTR,m)\n",
    "    DTR_PCA=P.T@DTR\n",
    "    DVAL_PCA=P.T@DVAL\n",
    "    #MVG model\n",
    "    hParams_mvg = Gau_MVG_ML_estimation(DTR_PCA, LTR)\n",
    "    llr_mvg = compute_llr_binary(DVAL_PCA, hParams_mvg)\n",
    "    predictions_mvg = compute_prediction_from_llr_binary(llr_mvg)#trashold=0\n",
    "    error_rate_mvg = compute_error_rate(predictions_mvg, LVAL)\n",
    "    print(\"Error rate MVG: \", error_rate_mvg)  \n",
    "    print(\"Accuracy MVG: \", 100 - error_rate_mvg) \n",
    "    print(\"\")\n",
    "    #Tied model\n",
    "    hParams_tied = Gau_Tied_ML_estimation(DTR_PCA, LTR)\n",
    "    llr_tied = compute_llr_binary(DVAL_PCA, hParams_tied)\n",
    "    predictions_tied = compute_prediction_from_llr_binary(llr_tied)#trashold=0\n",
    "    error_rate_tied = compute_error_rate(predictions_tied, LVAL)\n",
    "    print(\"Error rate Tied: \", error_rate_tied)\n",
    "    print(\"Accuracy Tied: \", 100 - error_rate_tied)\n",
    "    print(\"\")\n",
    "    #Naive Bayes model\n",
    "    hParams_naive = Gau_NaiveBayes_ML_estimation(DTR_PCA, LTR)\n",
    "    llr_naive = compute_llr_binary(DVAL_PCA, hParams_naive)\n",
    "    predictions_naive = compute_prediction_from_llr_binary(llr_naive)#trashold=0\n",
    "    error_rate_naive = compute_error_rate(predictions_naive, LVAL)\n",
    "    print(\"Error rate Naive: \", error_rate_naive)\n",
    "    print(\"Accuracy Naive: \", 100 - error_rate_naive)\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe718e4d",
   "metadata": {},
   "source": [
    "\n",
    "When examining the results across different PCA dimensions (m=1 to m=6):\n",
    "\n",
    "For the MVG model:\n",
    "- Performance gradually improves from m=1 (90.75%) to m=6 (93.0%)\n",
    "- Even with just one principal component, the model achieves over 90% accuracy\n",
    "- The highest accuracy with m=6 matches the performance using original features (93.0%)\n",
    "\n",
    "For the Tied Gaussian model:\n",
    "- Performance remains nearly constant across different PCA dimensions (around 90.7%)\n",
    "- Dimensionality reduction shows no clear benefit for this model\n",
    "\n",
    "For the Naive Bayes model:\n",
    "- Performance with PCA varies between 90.75% and 91.25%\n",
    "- The model performs significantly better with original features (92.8%) than with PCA-transformed features at full dimensionality (91.1%)\n",
    "- This suggests PCA creates feature dependencies that conflict with the Naive Bayes independence assumption\n",
    "\n",
    "**Is PCA effective for this dataset with Gaussian models?**\n",
    "PCA does not improve classification accuracy for this dataset. However, it offers effective dimensionality reduction with minimal performance loss, particularly for the MVG model. With just 5 PCA components, the MVG model achieves 92.9% accuracy, very close to the 93.0% with all features. For the Naive Bayes model, PCA actually reduces performance by transforming the relatively independent original features into dependent PCA components.\n",
    "\n",
    "**Overall, which model provided the best accuracy on the validation set?**\n",
    "The MVG model using original features achieved the highest accuracy at 93.0%. The Naive Bayes model with original features performed nearly as well at 92.8%, while the Tied model achieved 90.7%. This suggests that for this dataset, capturing the class-specific covariance structure is important for optimal classification performance.\n",
    "\n",
    "In conclusion, while PCA doesn't improve the best model's performance, it demonstrates that most discriminative information can be captured in fewer dimensions, offering potential benefits for computational efficiency and visualization without significant accuracy loss."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
